ğŸ¤° Maternal Health Risk Prediction

ğŸ“Œ Introduction

ğŸ“ Problem statement: Predict the maternal health risk category (low, mid, high) for pregnant women using clinical features (age, blood pressure, blood sugar, body temperature, heart rate, etc.).

ğŸ¯ Objective: Build and compare classification models to predict maternal health risk and identify important predictors for early intervention.

ğŸ“Š Dataset: It uses a tabular dataset containing clinical measurements for pregnant women. Key features include Age, SystolicBP, DiastolicBP, BS (blood sugar), BodyTemp, HeartRate, and a target RiskLevel with categories: low risk, mid risk, high risk.

( Dataset taken from Kaggle. )

ğŸ” Steps of Analysis
1ï¸âƒ£ Data Loading & Inspection

ğŸ“¥ Read dataset into DataFrame & view first rows

ğŸ” Check data types, missing values, basic stats

2ï¸âƒ£ Data Cleaning & Preprocessing

 Handle missing values

Encode categorical target (RiskLevel)

Scale features (if needed)

Train/test split

3ï¸âƒ£ Exploratory Data Analysis (EDA)

ğŸ“¦ Class distribution for RiskLevel

ğŸ“ˆ Histograms & Boxplots â†’ feature distributions & outliers

ğŸ”¥ Correlation Heatmap â†’ check multicollinearity

ğŸ¨ Pairplots/Scatter plots â†’ class separation

4ï¸âƒ£ Model Building

Trained multiple classification models: Logistic Regression, Decision Tree Classifier, Random Forest Classifier, Support Vector Classifier (SVC) 

For some models, multiple hyperparameter variants were evaluated (e.g. for Random Forest & Decision Tree).


5ï¸âƒ£ Evaluation

ğŸ“‘ Metrics: Accuracy, Precision, Recall, F1-score

ğŸŸ¦ Confusion Matrices with plots

ğŸ“Š Feature Importance (Random Forest)

ğŸ“ˆ Results

Logistic Regression: 61% 

Decision Tree: 86% 

Random Forest: 87â€“88% (Best performer)

SVC: 67% 

ğŸ” Key Takeaways:

Overall findings: Random Forest performed best in these experiments (up to ~88% accuracy), meaning it made the fewest overall mistakes when predicting the three risk categories on the test set. Logistic Regression performed worst among the reported runs (61% accuracy), which suggests the relationship between features and risk level is not purely linear. 

Class-wise performance: The classification reports show that some classes (for example, mid risk) may have lower recall or f1-score in certain models â€” indicating those cases are harder for the model to correctly identify. The low-risk and high-risk classes generally achieved better precision/recall depending on the model.
Random Forest gave the best accuracy (~88%) 

ğŸ“Š Explanation of Key Graphs

 Class Distribution Bar Chart â†’ shows balance/imbalance

ğŸ“ˆ Histograms & Boxplots â†’ reveal skewness/outliers (e.g., BP extremes)

 Correlation Heatmap â†’ detect multicollinearity

ğŸ¨ Pairplots â†’ check feature separability by RiskLevel

Feature Importance (RF) â†’ SystolicBP, Age, etc., most predictive

Confusion Matrix â†’ shows which classes are confused (e.g., mid risk â†’ low risk)

ğŸ“ Model Performance Metrics

Accuracy: 0.61, 0.86, 0.87, 0.88, 0.67

Precision: Correctness of positive predictions

Recall: Coverage of actual positives

F1-Score: Balance of precision & recall

ğŸ’¡ Insight: A high precision but low recall on high risk â†’ model misses cases but avoids false alarms

ğŸ› ï¸ Technologies Used

 Python 3

ğŸ“¦ Libraries:

pandas, numpy â†’ data handling

matplotlib, seaborn â†’ visualization

scikit-learn â†’ models & evaluation
